{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Implementation From Scratch\n",
    "\n",
    "* **Logistic with SGD, CrossEntropy**\n",
    "* **Naive Bayes with Bernoulli, Multinomial, Gaussian**\n",
    "\n",
    "\n",
    "Data used: \n",
    "* [Adult dataset/Census Income](https://archive.ics.uci.edu/ml/datasets/Adult)\n",
    "* [Ionosphere](https://archive.ics.uci.edu/ml/datasets/ionosphere)\n",
    "* [Breast Cancer Wisconsin](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)\n",
    "* [Wine Quality](https://archive.ics.uci.edu/ml/datasets/Wine+Quality)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages installation and data read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "data = pd.read_csv(\"C:/Users/ldhan/OneDrive/Desktop/adult.data\", delimiter=\",\", header = None)\n",
    "\n",
    "X = data.iloc[:,0:-1]\n",
    "y = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistics\n",
    "\n",
    "SGD <br />\n",
    "Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic:\n",
    "    def __init__(self, learning_rate, n_iter, intercept = True):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "        self.intercept = intercept\n",
    "\n",
    "    # Sigmoid function provided by slides\n",
    "    # y_h\n",
    "    def sigmoid(self, X, weight):\n",
    "        z = np.dot(X, weight)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def cost(self, weight, X, y):\n",
    "        # h = np.dot(X, weight)\n",
    "        # cost = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "            # Computes the cost function for all the training samples\n",
    "        m = X.shape[0]\n",
    "        cost = -(1 / m) * np.sum(y * np.log(self.sigmoid(X, weight)) + (1 - y) * np.log(\n",
    "                1 - self.sigmoid(X,weight)))\n",
    "        return cost\n",
    "\n",
    "    def gradientDescent(self, X, y_h, y, weight):\n",
    "        gradient = np.dot(X.T, (y_h - y)) / len(X)\n",
    "        weight -= self.learning_rate * gradient\n",
    "        return weight\n",
    "\n",
    "    # CrossEntropyLoss formula provided by slides\n",
    "    # def loss(self, y_h, y):\n",
    "    #     loss = np.mean((-y * np.log(y_h) - (1 - y) * np.log(1 - y_h)))\n",
    "    #     return loss\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.intercept == True:\n",
    "            X = np.concatenate((np.ones((X.shape[0], 1)),X), axis = 1)\n",
    "\n",
    "        # Weights\n",
    "        self.weight = np.zeros(X.shape[1])\n",
    "        # y_hat = self.sigmoid(X, theta)\n",
    "        acc = []\n",
    "        loss = []\n",
    "        for i in range(self.n_iter):\n",
    "            y_h = self.sigmoid(X, self.weight)\n",
    "            self.weight = self.gradientDescent(X, y_h, y, self.weight)\n",
    "            acc.append(np.mean((self.sigmoid(X, self.weight)>=0.5)==y))\n",
    "            loss.append(self.cost(self.weight, X, y))\n",
    "    def predict(self, X):\n",
    "        if self.intercept == True:\n",
    "            X = np.concatenate((np.ones((X.shape[0], 1)),X), axis = 1)\n",
    "        return self.sigmoid(X, self.weight) >= 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "Bernoulli <br />Multinomial<br />Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class naiveBayes:\n",
    "        def __init__(self):\n",
    "            return None\n",
    "\n",
    "        def BernoulliNaiveBayes(self, prior, likelihood, X,\n",
    "            ):\n",
    "            log_p = np.log(prior) + np.sum(np.log(likelihood) * X[:,None], 0) + \\\n",
    "            np.sum(np.log(1-likelihood) * (1 - X[:,None]), 0)\n",
    "            log_p -= np.max(log_p) #numerical stability\n",
    "            posterior = np.exp(log_p) # vector of size 2\n",
    "            posterior /= np.sum(posterior) # normalize\n",
    "            return posterior # posterior class probability\n",
    "\n",
    "        def MultinomialNaiveBayes(self, X, y):\n",
    "            '''\n",
    "            calculate every P(...|class) by freq\n",
    "            '''\n",
    "\n",
    "            # categorical = [i for i in range(0,len(X_test.columns)) if X_test.iloc[:,i].dtype == \"object\"]\n",
    "            prior = pd.DataFrame(X).groupby(y).agg(lambda x: len(x) / len(X)).iloc[:,0].astype('float64') # Income Prior\n",
    "            # likelihood for each p(...|class), formula from slides.\n",
    "            likelihood = [pd.DataFrame(y).groupby([y, X.iloc[:,i]]).agg(lambda x: len(x)).astype('float64').iloc[:,0] for i in range(len(self.categorical))]\n",
    "            log_likelihood = likelihood.copy()\n",
    "\n",
    "            for i in range(len(self.categorical)):\n",
    "                for j in np.unique(y):\n",
    "                    log_likelihood[i][j] = np.log(pd.DataFrame(X).groupby(y).agg(lambda x: len(x) / len(X)).iloc[:,0]\\\n",
    "                    .astype('float64')[j]+(likelihood[i][j]/pd.DataFrame(y).groupby(y).count().iloc[:,0][j])) # prior + likelihood\n",
    "            log_likelihood_reordered = [i.reorder_levels([1,0]) for i in log_likelihood] # for future easy indexing use\n",
    "            return log_likelihood_reordered\n",
    "\n",
    "\n",
    "        def GaussianNaiveBayes(self, X, # N x D\n",
    "            y):\n",
    "            # From slides\n",
    "            X = X.values\n",
    "            y = pd.get_dummies(y).values\n",
    "            N,C = y.shape\n",
    "            D = X.shape[1]\n",
    "            self.mu, self.s = np.zeros((C,D)), np.zeros((C,D))\n",
    "            for c in range(C): #calculate mean and std\n",
    "                inds = np.nonzero(y[:,c])[0]\n",
    "                self.mu[c,:] = np.mean(X[inds,:], 0)\n",
    "                self.s[c,:] = np.std(X[inds,:], 0)\n",
    "                self.log_prior = np.log(np.mean(y, 0))[:,None]\n",
    "\n",
    "            return self.mu, self.s, self.log_prior\n",
    "\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            '''\n",
    "            Prepare parameters, calculate from previous functions, based on the training data, namely, freq calculated based upon training set\n",
    "            '''\n",
    "            self.X = pd.DataFrame(X)\n",
    "            self.y = y\n",
    "            if len(np.unique(X.dtypes))<=0:\n",
    "                return None\n",
    "\n",
    "            if len(np.unique(X.dtypes))>=1:\n",
    "                self.categorical = [i for i in range(0,len(X.columns)) if X.iloc[:,i].dtype == \"O\" or X.iloc[:,i].dtype == \"object\"] # categorical features\n",
    "                self.continous = [i for i in range(0,len(X.columns)) if X.iloc[:,i].dtype == \"int64\" or X.iloc[:,i].dtype == \"float64\"]\n",
    "                X_cate = X.iloc[:,self.categorical]\n",
    "                X_cont = X.iloc[:,self.continous]\n",
    "                if len(X_cate.columns) + len(X_cont.columns) != len(X.columns):\n",
    "                    raise (\"Mannually choose categorical and continous features\")\n",
    "                if not X_cont.empty:\n",
    "                    self.mu, self.s, self.log_prior = self.GaussianNaiveBayes(X_cont, self.y)\n",
    "        def predict(self, Xtest):\n",
    "            '''\n",
    "            After fitting, we have training set's parameters handy, now, use test set to make the prediction in accordance with the likelihood obtained above\n",
    "            '''\n",
    "            Xtest = pd.DataFrame(Xtest)\n",
    "            X_test_cate = Xtest.iloc[:,self.categorical]\n",
    "            X_test_cont = Xtest.iloc[:,self.continous]\n",
    "            X_cate = self.X.iloc[:,self.categorical]\n",
    "            X_cont = self.X.iloc[:,self.continous]\n",
    "            multinomial = np.zeros((len(X_test_cate), len(np.unique(self.y))))\n",
    "            gaussian = np.zeros((len(np.unique(y)),len(X_test_cate)))\n",
    "            if not X_cate.empty:\n",
    "                log_likelihood_reordered = self.MultinomialNaiveBayes(X_cate, self.y) # get log likelihood table from training set\n",
    "                for i in range(len(X_test_cate)):\n",
    "                    for j in range(len(X_test_cate.columns)):\n",
    "                        multinomial[i] += np.array(log_likelihood_reordered[j][X_test_cate.values[i][j]]) # log likelihood table from test set N*(n of classes)\n",
    "            if not X_cont.empty:\n",
    "                if np.isnan(np.log(self.s[:,None,:])).any():\n",
    "                    raise ValueError(\"Check data input for 'zero', leads to log(0) => nan\")\n",
    "                log_likelihood = - np.sum(np.log(self.s[:,None,:]) +.5*(((X_test_cont.values[None,:,:]- self.mu[:,None,:])/self.s[:,None,:])**2), 2) # from lecture, gaussian log likelihood\n",
    "                gaussian = self.log_prior + log_likelihood\n",
    "            predictions = np.argmax(multinomial + gaussian.T, axis = 1) # combine gaussian and multinomial in mix type of features data, get the max\n",
    "            return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results/Predictions Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_normal(data):\n",
    "    # Mean normalization\n",
    "    return (data - data.mean())/ data.std()\n",
    "\n",
    "def min_max_normal(data):\n",
    "    # Min-max normalization\n",
    "    return (data-data.min())/(data.max()-data.min())\n",
    "\n",
    "def evaluate_acc(y_h, y):\n",
    "    # self-correction\n",
    "    if np.mean(y_h == y) < 0.5:\n",
    "        # print(y_h, y)\n",
    "        acc = 1 - np.mean(y_h == y)\n",
    "    return np.mean(y_h == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model by Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_train(fit, X, y, k, learning_rate = None, n_iter = None, intercept = True):\n",
    "    '''CV training, with k-folder and k times of training'''\n",
    "    if fit == \"logistic\":\n",
    "        model = Logistic(learning_rate = learning_rate, n_iter = n_iter, intercept = intercept)\n",
    "    else:\n",
    "        model = naiveBayes()\n",
    "    training_accuracy = []\n",
    "    validation_accuracy = []\n",
    "    start = list(range(len(X.index)))\n",
    "    i = 0\n",
    "    while i<k :\n",
    "        # try: because class in training set may not in test set\n",
    "        try:\n",
    "            index = np.random.choice(start, replace = False, size = int((len(X.index))*((k-1)/k)))\n",
    "            # Avoid duplicate\n",
    "            val_index = list(set(start) - set(index))\n",
    "            # Training\n",
    "            model.fit(X.iloc[index], y[index])\n",
    "            # validation accuracy\n",
    "            # Choose the one portion of the k-folded data\n",
    "            validation_accuracy.append(evaluate_acc(model.predict(X.iloc[val_index]), y[val_index]))\n",
    "            # Training accuracy\n",
    "            training_accuracy.append(evaluate_acc(model.predict(X.iloc[index]), y[index]))\n",
    "            i += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return validation_accuracy, training_accuracy, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, recall, accuracy, Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_score(y_pred, y_true):\n",
    "    return ((y_true==1)*(y_pred==1)).sum()/(y_pred==1).sum()\n",
    "def recall_score(y_pred, y_true):\n",
    "    return ((y_true==1)*(y_pred==1)).sum()/(y_true==1).sum()\n",
    "def f1_score(y_pred, y_true):\n",
    "    num = 2*precision_score(y_true, y_pred)*recall_score(y_true, y_pred)\n",
    "    deno = (precision_score(y_true, y_pred)+recall_score(y_true, y_pred))\n",
    "    return num/deno\n",
    "\n",
    "def confusion_matrix(y_h, y):\n",
    "    # Confusion matrix\n",
    "    return pd.crosstab(pd.Series(y_h, name = 'prediction'), pd.Series(y, name = \"true\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Validation Accuracy: 0.8433594349762015\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    X_adult = pd.get_dummies(X) # Char / calegratial to one-hot encoding\n",
    "    y_adult = np.asarray([0 if i == \" <=50K\" else 1 for i in y]) # Char to numerical\n",
    "    y_ionosphere = np.array([0 if i == \"b\" else 1 for i in y])\n",
    "    y_wine = np.array([0 if i == 3 or i == 4 or i == 5 else 1 for i in y])\n",
    "    y_wdbc = np.array([0 if i == \"M\" else 1 for i in y])\n",
    "\n",
    "    # Cross Validation\n",
    "    k = 5\n",
    "    # NB is slower than Logistics\n",
    "\n",
    "    # Higher accuracy with application of normalization\n",
    "    # say, mean_normal(X) or min_max_normal(X)\n",
    "    start = time.time()\n",
    "    log_validation_accuracy, log_training_accuracy, model_log = cross_validation_train(\"logistic\",mean_normal(X_adult.iloc[:,:]), y_adult, k, learning_rate = 0.01, n_iter = 2000, intercept = True)\n",
    "    time.time() - start\n",
    "    start = time.time()\n",
    "    nb_validation_accuracy, nb_training_accuracy, model_nb = cross_validation_train(\"NB\",X, y_adult, k)\n",
    "    time.time() - start\n",
    "    confusion_matrix([1 if i == True else 0 for i in model_log.predict(mean_normal((X_adult.iloc[:,:])))], y_adult)\n",
    "    # data = pd.read_csv(\"C:/Users/Donghan/Desktop/551A1/winequality-red.data\", delimiter=\";\")\n",
    "    # Recall, precision, F-1 score\n",
    "    def eval(model, X, y):\n",
    "        return precision_score(np.array([1 if i == True else 0 \\\n",
    "                                for i in model.predict(mean_normal(pd.get_dummies(X)))]),\\\n",
    "                               y),recall_score(np.array([1 if i == True else 0 for i in \\\n",
    "                                model.predict(mean_normal(pd.get_dummies(X)))]), y),\\\n",
    "                                f1_score(np.array([1 if i == True else 0 for i in model.predict(\\\n",
    "                                mean_normal(pd.get_dummies(X)))]), y)\n",
    "\n",
    "    eval(model_log, X_adult.iloc[:,:], y_adult)\n",
    "\n",
    "\n",
    "\n",
    "    # Feature importance for wine data\n",
    "    dict(zip(model_log.weight,[\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\",\"quality\"]))\n",
    "\n",
    "\n",
    "#     data = data.drop([\"citric acid\", \"density\",\"pH\"], axis = 1)\n",
    "#     X = data.iloc[:,0:-1]\n",
    "#     y = data.iloc[:,-1]\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Logistic Validation Accuracy: {}\".format(np.mean(log_validation_accuracy)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Validation Accuracy: 0.8095194226930754\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes Validation Accuracy: {}\".format(np.mean(nb_validation_accuracy)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
